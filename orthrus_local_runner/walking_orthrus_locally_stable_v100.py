# -*- coding: utf-8 -*-
"""walking_orthrus_locally_stable_v100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1__ajInj0i6vjhyakUhw0NeUUljGV-6p6
"""

#!/usr/bin/env python3

from __future__ import annotations

# Python standards
import argparse
import datetime
import glob
import gzip
import io
import json
import logging
import os
import re
import shutil
import subprocess
import sys
import time
import warnings
from contextlib import redirect_stderr, redirect_stdout
from itertools import chain
from typing import Dict, List, Optional, Set

# 3rd part from pip etc
import mokapot
import numpy as np
import pandas as pd
import requests
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from joblib import Parallel, delayed
from pyteomics import mztab
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier


# ----------------------------- Utility Setup ------------------------------- #


# for logging
LOGGER = logging.getLogger("orthrus_metaproteomics")
warnings.filterwarnings("ignore")

def initiate_logger(level: str = "INFO",
                    log_dir: str = "."):
    numeric = getattr(logging, level.upper(), logging.INFO)
    logger = LOGGER
    logger.setLevel(numeric)
    logger.handlers.clear()
    logger.propagate = False

    h = logging.StreamHandler()
    h.setLevel(numeric)
    h.setFormatter(logging.Formatter(
        "%(asctime)s | \x1b[1;36m%(name)s\x1b[0m | %(levelname)s | %(message)s",
        "%Y-%m-%d %H:%M:%S",
    ))
    logger.addHandler(h)

    os.makedirs(log_dir, exist_ok=True)
    ts = datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    log_path = os.path.join(log_dir, f"orthrus_metaproteomics_{ts}.log")
    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh_fmt = logging.Formatter(
        "%(asctime)s | %(name)s | %(levelname)s | %(message)s",
        "%Y-%m-%d %H:%M:%S",
    )
    fh.setFormatter(fh_fmt)
    logger.addHandler(fh)

    logging.getLogger("mokapot").setLevel(logging.ERROR)

    return logger


# Casanovo 4x column names
mztab_seq_col = 'sequence'
score_col = 'search_engine_score[1]'
naked_pattern = re.compile(r'(?:(?<=[A-Z])[+-]?\d+(?:\.\d+)?|^[+-]?\d+(?:\.\d+)?)')


# mztab -> pd.Dataframe
def prep_mztab(mztab_path: str):
    LOGGER.info("Reading mzTab: %s", mztab_path)
    m = mztab.MzTab(mztab_path)
    df = m.spectrum_match_table
    if df is None or df.empty:
        raise ValueError(f"{mztab_path} is empty")
    if mztab_seq_col not in df.columns:
        raise KeyError(f"'{mztab_seq_col}' column is missing in the file: {mztab_path}")
    df1 = df.reset_index(drop=True)
    df2 = df1.assign(sequence_naked=df1[mztab_seq_col].str.replace(naked_pattern, '', regex=True))
    df3= df2.assign(nAA=df2['sequence_naked'].str.len())
    df4=df3.sort_values(by='sequence_naked').drop_duplicates(subset='sequence_naked', keep="first").reset_index(drop=True)
    LOGGER.debug("mzTab prepped: %d unique naked sequences", df4.shape[0])
    return df4


# fasta -> pd.Dataframe
def fasta_to_df(fasta_file: str):
    LOGGER.info("Reading FASTA: %s", fasta_file)
    data = []
    for record in SeqIO.parse(fasta_file, "fasta"):
        protein_id = record.id
        description = record.description
        sequence = str(record.seq)
        if not sequence:
            raise ValueError(f"Record with ID '{protein_id}' has no sequence in the fasta file.")
        data.append((protein_id, description, sequence))
    df = pd.DataFrame(data, columns=["Protein_ID", "Description", "Sequence"])
    df1=df.assign(UniProt_ID=df['Protein_ID'].str.split('|').str[1])
    LOGGER.debug("FASTA prepped: %d protein entries", df1.shape[0])
    return df1


# filter casanovo outputs using the maximun score below zero
def casa_filter (df):
    if score_col not in df.columns:
        raise KeyError(f"'{score_col}' not found")
    np_array = df[score_col].to_numpy()
    max_below_zero = np_array[np_array < 0].max()
    df1=df[df[score_col]>=max_below_zero]
    LOGGER.info(
        "casa_filter: kept %d/%d PSMs (threshold=%.6f)",
        df1.shape[0],
        df.shape[0],
        max_below_zero)
    return df1


# ----------------------------- matching & Bayes ranking ------------------------------- #


#prepare overlapping sequence tags for string matching
def get_seq_tags (sequence: str, k: int):
    return set(sequence[i:i+k] for i in range(len(sequence) - k + 1))


#change chunk size here for memory if needed
def matching_count_v5 (fasta_df: pd.DataFrame, casanovo_df: pd.DataFrame, k: int, chunk_size: int=10000):
    LOGGER.info("Generating sequence tags (k=%d)...", k)
    sequence_set = get_seq_tags(''.join(chain.from_iterable(casanovo_df['sequence_naked'].astype(str))), k)
    LOGGER.info("Generated %d unique tags from Casanovo outputs.", len(sequence_set))
    result_df = pd.DataFrame()
    for start in range(0, len(fasta_df), chunk_size):
        chunk = fasta_df.iloc[start:start+chunk_size].copy()
        chunk['seq_tags'] = chunk['Sequence'].astype(str).str.replace('I', 'L').apply(lambda x: get_seq_tags(x, k))
        chunk['matched_count'] = chunk['seq_tags'].apply(lambda seq_tags: len(seq_tags & sequence_set))
        chunk = chunk.assign(matched=chunk['matched_count'].apply(lambda x: 1 if x >= 2 else 0))
        result_df = pd.concat([result_df, chunk], ignore_index=True)
    LOGGER.info(
        "Tag matching complete: total matched tag counts=%d",
        int(result_df["matched_count"].sum()))
    return result_df


#get tryptic peptides per database entry
def count_tryptic_peptides(sequence: str):
    pattern=r'(?<=[KR])'
    peptides = re.split(pattern, sequence)
    filtered_peptides = [peptide for peptide in peptides if len(peptide) >= 6]
    return len(filtered_peptides)


#prepare a dataframe for NB classification
def prep_Bayes (df: pd.DataFrame):
    df1=df.assign(length=df['Sequence'].astype(str).str.len(),
                 tryptic_count=df['Sequence'].apply(count_tryptic_peptides),
                 tag_count=df['seq_tags'].apply(len))
    df2=df1.assign(SAF=df1['matched_count']/df1['length'],
                 try_ratio=df1['tryptic_count']/df1['tag_count'])
    return df2


# bayes ranking
def get_bayes_ranking_test (df: pd.DataFrame, threshold: float = 0.95):
    m=prep_Bayes(df)
    required_columns = {'SAF', 'try_ratio', 'matched'}
    if not required_columns.issubset(m.columns):
        missing = required_columns - set(m.columns)
        raise ValueError(f"Missing columns in DataFrame: {missing}")
    m1 = m[m['tag_count']>0]
    X = m1[['SAF', 'try_ratio']].to_numpy()
    y = m1['matched'].to_numpy()
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(*X.shape)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=7)
    gnb = GaussianNB()
    gnb.fit(X_train, y_train)
    y_pred = gnb.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    LOGGER.info("GaussianNB ▶ accuracy=%.4f, precision=%.4f, f1=%.4f", accuracy, precision, f1)
    whole_pred = gnb.predict(X_scaled)
    class_probabilities = gnb.predict_proba(X_scaled)
    m2 = m1.assign(pred=class_probabilities[:, 1])
    m3 = m2[m2['pred']>=threshold]
    LOGGER.info("Shortlisted %d proteins at ≥ %.2f.", m3.shape[0], threshold)
    return m3


# ----------------------------- De novo -> sample-specific .fastas ------------------------------- #


def matching_ranking_to_fasta_v5 (mztab_path: str, fasta_df: pd.DataFrame):
    p = prep_mztab(mztab_path)
    p1 = casa_filter(p)
    k = int(p1['nAA'].median())
    m = matching_count_v5 (fasta_df, p1, k, chunk_size=10000)
    m1 = get_bayes_ranking_test (m)
    seq_records = []
    for index, row in m1.iterrows():
        header_id = f"{row['Description']}"
        sequence = Seq(row['Sequence'])
        description = ""
        seq_record = SeqRecord(sequence, id=header_id, description=description)
        seq_records.append(seq_record)

    output_fasta_filepath = mztab_path.replace('.mztab', '_matched.fasta')

    with open(output_fasta_filepath, 'w') as output_file:
        SeqIO.write(seq_records, output_file, 'fasta')
    LOGGER.info("Wrote matched FASTA %s (entries=%d)", output_fasta_filepath, m1.shape[0])


def process_all_mztab_files_v2 (folder_path: str, database_path: str):
    mztab_filepaths = glob.glob(f"{folder_path}/*.mztab")
    LOGGER.info("Found %d mzTab file(s) in %s", len(mztab_filepaths), folder_path)
    fasta_df=fasta_to_df(database_path)
    LOGGER.info("Reference FASTA loaded from %s (proteins=%d)", database_path, fasta_df.shape[0])
    for mztab in mztab_filepaths:
        matching_ranking_to_fasta_v5 (mztab, fasta_df)


# ----------------------------- utilities for Sage ------------------------------- #


def organise_files(directory: str, file_type: str):
    if not os.path.isdir(directory):
        log.error("Directory does not exist: %s", directory)
        return

    MS2_files = glob.glob(os.path.join(directory, f'*.{file_type}'))
    for MS2 in MS2_files:
        log.info("Organising files in %s ...", directory)
        base_name = os.path.splitext(os.path.basename(MS2))[0]
        new_folder_path = os.path.join(directory, base_name)
        if not os.path.exists(new_folder_path):
            os.makedirs(new_folder_path)

        MS2_path = os.path.join(new_folder_path, os.path.basename(MS2))
        if not os.path.exists(MS2_path):
            shutil.move(MS2, new_folder_path)
            log.info("Moved %s to %s", MS2, new_folder_path)
        else:
            log.warning("MS2 file already exists in the destination: %s", MS2_path)

        fasta_filename = f"{base_name}_matched.fasta"
        fasta_file = os.path.join(directory, fasta_filename)
        if os.path.exists(fasta_file):
            new_fasta_path = os.path.join(new_folder_path, fasta_filename)
            if not os.path.exists(new_fasta_path):
                shutil.move(fasta_file, new_folder_path)
                log.info("Moved %s to %s", fasta_file, new_folder_path)
            else:
                log.info(".fasta file already exists in the destination: %s", new_fasta_path)
        else:
            log.warning("No matching .fasta file found for %s", base_name)


def get_sage_config(json_file_path: str,
                    peak_folder: str,
                    static_mods: Dict[str, float],
                    new_mods: Dict[str, List[float]],
                    missed_cleavages: int,
                    enzyme: str,
                    min_len: int,
                    max_len: int,
                    max_variable_mods: int,
                    output_config_path: str):

    with open(json_file_path, 'r') as file:
        json_data = json.load(file)
        peak_files = glob.glob(peak_folder)
        LOGGER.info(f"{len(peak_files)} file(s) collected from {peak_folder}")

        json_data['mzml_paths'] = peak_files
        json_data['database']['static_mods'] = static_mods
        json_data['database']['variable_mods'] = new_mods
        json_data['database']['enzyme']['missed_cleavages'] = missed_cleavages
        json_data['database']['enzyme']['cleave_at']= enzyme
        json_data['database']['enzyme']['min_len'] = min_len
        json_data['database']['enzyme']['max_len'] = max_len
        json_data['database']['max_variable_mods'] = max_variable_mods
        json_data['database']['decoy_tag'] = "rev_"
        json_data['database']['generate_decoys'] = True

    with open(output_config_path, 'w') as f:
        json.dump(json_data, f, indent=4)
    LOGGER.info("Wrote Sage config: %s", output_config_path)


#Create a PTM dict -> {'M':[15.994915], 'N':[0.984016]}. Skipping None
def build_ptms_from_lists(aas: Optional[List[str]],
                          mods: Optional[List[str]]):
    ptms: Dict[str, List[float]] = {}
    for aa, mod in zip(aas or [], mods or []):
        if aa and aa != "None":
            ptms.setdefault(aa, []).append(float(mod))
    return ptms


# ----------------------------- Run Casanovo ------------------------------- #


def run_casanovo (folder_path: str,
                  file_type: str,
                  use_default: bool,
                  model: Optional[str] = None,
                  config: Optional[str] = None):
    files = glob.glob(f"{folder_path}/*.{file_type}")
    if not files:
        LOGGER.error("No instrument files found in %s with extension .%s", folder_path, file_type)
        sys.exit(1)

    env = os.environ.copy()
    env["TF_CPP_MIN_LOG_LEVEL"] = "2"
    env["TF_ENABLE_ONEDNN_OPTS"] = "0"

    for instrument_file in files:
        output_path=instrument_file.replace(f".{file_type}", ".mztab")
        if use_default:
            cmd = ["casanovo", "sequence", instrument_file, "-v", "info", "-o", output_path]
        else:
            cmd= ["casanovo", "sequence", instrument_file, "-m", model, "-c", config, "-v", "info", "-o", output_path]
        LOGGER.info("Running Casanovo: %s", " ".join(cmd))
        subprocess.run(cmd,
                       check=True,
                       stdout=subprocess.DEVNULL,
                       stderr=subprocess.DEVNULL,
                       env=env)


# ----------------------------- Run sage ------------------------------- #


def run_sage (
        folder_path: str,
        file_type: str,
        sage_path: str,
        json_file_path: str,
        enzyme: str,
        use_PTM_plus: bool,
        missed_cleavages: int,
        max_variable_mods: int,
        static_CAM: bool,
        aa_list: Optional[List[str]] = None,
        mod_list:  Optional[List[str]] = None):

    organise_files(folder_path, file_type)

    # constants
    min_len, max_len = 6, 30
    static_mods = {"C": 57.021464} if static_CAM else {}
    missed_cleavages = 2 if not use_PTM_plus else missed_cleavages
    max_variable_mods = 3 if not use_PTM_plus else max_variable_mods
    enzyme = enzyme
    PTMs = build_ptms_from_lists(aa_list, mod_list) if use_PTM_plus else {"M": [15.994915], "N": [0.984016], "Q": [0.984016]}

    # iterate each sub-folder
    big_folder = [p for p in glob.glob(f"{folder_path}/*") if os.path.isdir(p)]
    if not big_folder:
        warnings.warn("No subfolders found after organise_files().", RuntimeWarning)
        return

    for folder in big_folder:
        files = glob.glob(f"{folder}/*.{file_type}")
        if not files:
            LOGGER.warning("No %s files in %s; skipping.", file_type, folder)
            continue

        peak_path   = files[0]
        output_json = peak_path.replace(f".{file_type}", ".json")

        get_sage_config(
            json_file_path, peak_path, static_mods, PTMs,
            missed_cleavages, enzyme,
            min_len, max_len,
            max_variable_mods, output_json
        )

        fasta_files = glob.glob(f"{folder}/*.fasta")
        if not fasta_files:
            LOGGER.warning("No FASTA in %s; Sage will be skipped.", folder)

        fasta_path = fasta_files[0]
        cmd = [sage_path, output_json, "--fasta", fasta_path,
               "--write-pin",
               "--output_directory", folder
               ]
        LOGGER.info("Running Sage for %s", folder)
        subprocess.run(cmd,
                       check=True,
                       stdout=subprocess.DEVNULL,
                       stderr=subprocess.DEVNULL)


# ----------------------------- Brew mokapot ------------------------------- #


def get_all_pin_files(folder_path):
    psm_files = []
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.pin'):
                full_path = os.path.join(root, file)
                psm_files.append(full_path)
    return psm_files


def brew_mokapot (folder_path: str,
                  joint_modelling: bool,
                  default_Percolator: bool
                  ):
    #XGBoost schema from Fondrie & Noble (2021).A non-linear XGBoost seems to be better for rescoring open search results.
    grid = {
        "scale_pos_weight": np.logspace(0, 2, 3),
        "max_depth": [1, 3, 6],
        "min_child_weight": [1, 10, 100],
        "gamma": [0, 1, 10]}
    xgb_mod = GridSearchCV(
        XGBClassifier(),
        param_grid=grid,
        n_jobs=1,
        cv=3,
        scoring="roc_auc")

    if joint_modelling:
        psm_files = get_all_pin_files(folder_path)
        if not psm_files:
            warnings.warn("No .pin files found.", RuntimeWarning)
            return
        LOGGER.info("Brewing all .pins in folder: %s", folder_path)
        joint_psm_list = mokapot.read_pin(psm_files)
        model = mokapot.PercolatorModel() if default_Percolator else mokapot.Model(xgb_mod)
        with open(os.devnull, "w") as devnull, redirect_stdout(devnull), redirect_stderr(devnull):
            results, models = mokapot.brew(joint_psm_list, model)
        result_files = results.to_txt(folder_path)
        LOGGER.info("Mokapot (joint modelling) brewed")

    else:
        big_folder = [p for p in glob.glob(f"{folder_path}/*") if os.path.isdir(p)]
        for folder in big_folder:
            if not os.path.isdir(folder):
                continue
            LOGGER.info("Brewing folder: %s", folder)
            pin_files = glob.glob(f"{folder}/*.pin")
            if not pin_files:
                LOGGER.warning("No .pin files in %s; skipping.", folder)
                continue
            pin = pin_files[0]
            psm_list = mokapot.read_pin(pin)
            model = mokapot.PercolatorModel() if default_Percolator else mokapot.Model(xgb_mod)
            with open(os.devnull, "w") as devnull, redirect_stdout(devnull), redirect_stderr(devnull):
                results, models = mokapot.brew(psm_list, model)
            result_files = results.to_txt(folder)
            LOGGER.info("Mokapot (single model per experiment) brewed")


# ----------------------------- Main ------------------------------- #


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Running Orthrus for hybrid sequencing")

    # Casanovo config
    parser.add_argument("--folder_path", type=str, required=True, help="A single folder containing all instrument file(s)")
    parser.add_argument("--file_type", type=str, required=True, help="File extension, mzML or mgf (without dot)")
    parser.add_argument("--model", type=str, help="Path to a model (.ckpt)")
    parser.add_argument("--config", type=str, help="Path to a config file (.yaml)")
    parser.add_argument("--use_SwissProt", action="store_true", help="Download and use the latest SwissProt")
    parser.add_argument("--database_path", type=str, help="Path to a .fasta database if not use SwissProt")

    # Sage config
    parser.add_argument("--sage_path", type=str, required=True, help="Path to Sage binary")
    parser.add_argument("--json_file_path", type=str, required=True, help="Path to a default Sage json config file")
    parser.add_argument("--enzyme", type=str, default="KR", help="digestion enzyme, default='KR'")
    parser.add_argument("--missed_cleavages", type=int, default=2, help="Number of missed cleavages, default=2")
    parser.add_argument("--max_variable_mods", type=int, default=3, help="Maximum number of variable mods, default=3")
    parser.add_argument("--static_CAM", action="store_true", help="Apply Carbamidomethyl C (+57.021464)")
    parser.add_argument("--aas",  nargs="*", default=None, help="space separated list (no comma/brackets) of AAs for PTM+ -> P M Q")
    parser.add_argument("--mods", nargs="*", default=None, help="space separated list of Mods (no comma/brackets) (plz match the order of the provided AAs) -> 15.994915 15.994915 0.984016")

    # Mokapot config
    parser.add_argument("--joint_modelling", action="store_true", help="a combined model for low-abundance samples")
    parser.add_argument("--default_Percolator", action="store_true", help="Percolator SVM model, if not XGBClassifier")

    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging verbosity",
    )

    args = parser.parse_args()
    LOGGER = initiate_logger(args.log_level,
                             log_dir=args.folder_path)

    # validating args
    if (args.model and not args.config) or (args.config and not args.model):
        LOGGER.error("Provide both --model and --config, or omit both to use defaults.")
        sys.exit(1)
    if not os.path.isdir(args.folder_path):
        LOGGER.error("Folder does not exist: %s", args.folder_path)
        sys.exit(1)
    if not args.use_SwissProt and not args.database_path:
        LOGGER.error("Provide either --use_SwissProt or --database_path.")
        sys.exit(1)
    if args.database_path and not os.path.isfile(args.database_path):
        LOGGER.error("database_path not found: %s", args.database_path)
        sys.exit(1)


    # running casanovo
    use_default = not (args.model and args.config)

    run_casanovo(
        folder_path=args.folder_path,
        file_type=args.file_type,
        use_default=use_default,
        model=args.model,
        config=args.config,
    )

    if args.use_SwissProt:
        url = "https://ftp.uniprot.org/pub/databases/uniprot/knowledgebase/complete/uniprot_sprot.fasta.gz"
        output_file = "uniprot_sprot.fasta.gz"
        decompressed_file = "uniprot_sprot.fasta"
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            with open(output_file, 'wb') as f:
                shutil.copyfileobj(response.raw, f)
            LOGGER.info("%s downloaded successfully.", output_file)
        else:
            LOGGER.error("Failed to download %s. Status code: %d", output_file, response.status_code)
            sys.exit(1)
        with gzip.open(output_file, 'rb') as f_in:
            with open(decompressed_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        sprot_path="uniprot_sprot.fasta"
        process_all_mztab_files_v2(args.folder_path, sprot_path)
    else:
        process_all_mztab_files_v2(args.folder_path, args.database_path)


    # running sage
    auto_ptm_plus = bool(args.aas and args.mods)
    if bool(args.aas) ^ bool(args.mods):
        parser.error("Provide both --aas & --mods together.")
    if args.aas and args.mods and len(args.aas) != len(args.mods):
        parser.error("Provided AAs and mods not equal lengths")
    if not os.path.isfile(args.json_file_path):
        LOGGER.error("json_file_path not found: %s", args.json_file_path)
        sys.exit(1)

    run_sage(
        folder_path=args.folder_path,
        file_type=args.file_type,
        sage_path=args.sage_path,
        json_file_path=args.json_file_path,
        enzyme=args.enzyme,
        use_PTM_plus=auto_ptm_plus,
        missed_cleavages=args.missed_cleavages,
        max_variable_mods=args.max_variable_mods,
        static_CAM=args.static_CAM,
        aa_list=args.aas,
        mod_list=args.mods,
    )


    # brewing mokapot

    brew_mokapot(
        folder_path=args.folder_path,
        joint_modelling=args.joint_modelling,
        default_Percolator=args.default_Percolator,
    )