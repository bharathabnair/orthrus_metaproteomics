{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNHpsaVAQQjpm33Q4dB8HE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yc386/orthrus_metaproteomics/blob/main/Orthrus_stable_v100/orthrus_stable_v100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=19rmmQI1H2nIqgU598WROTcUNhOUoXcBP' width='400px' align='right'>\n",
        "\n",
        "# **Readme**\n",
        "\n",
        "---\n",
        "[Orthrus](https://www.biorxiv.org/content/10.1101/2024.11.15.623814v1) üêæ is a hybrid, two-software pipeline that integrates [Casanovo](https://github.com/Noble-Lab/casanovo) (an AI transformer) with [Sage](https://github.com/lazear/sage) (a fast database search engine with advanced features like retention time alignment and machine learning-based rescoring).\n",
        "\n",
        "Designed to handle large search spaces in metaproteomics and palaeoproteomics, Orthrus leverages *de novo* sequencing to define sample-specific databases, and uses probability ranking and conventional database searching to control FDRs (false discovery rates).\n",
        "\n",
        "Orthrus is optimised for online use on Google Colab ü•≥\n",
        "\n",
        "# **Quick start**‚ùóÔ∏è\n",
        "1. Before walking the dog, please change the runtime type to GPU (A100, L4, or T4. A100 most efficient but T4 is free)\n",
        "2. Click the folder image üóÇÔ∏è on the left and mount your Google drive (permission pending)\n",
        "3. Click `File` (top left) to save a local copy\n",
        "4. **Run the `Install everything, will automatically restart` cell first** and wait for restarting, to resolve the numpy+pandas version conflicts. Casanovo 4x and Mokapot require numpy 1x\n",
        "5. Choose, Casanovo, Sage, and Mokapot configurations. Then **from the `Configure Casanovo` cell, cick `Runtime` -> `Run cell and below`**"
      ],
      "metadata": {
        "id": "PprpvHaeBPgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install everything, will **automatically restart** to resolve version conflicts\n",
        "\n",
        "import os, sys, subprocess, time, IPython\n",
        "from pathlib import Path\n",
        "if not Path(\"Orthrus_READY\").exists():\n",
        "    print(\"installing conda and packagesüì¶ ‚¨áÔ∏è\")\n",
        "    #Sage version 0.14.7\n",
        "    !wget -q https://github.com/lazear/sage/releases/download/v0.14.7/sage-v0.14.7-x86_64-unknown-linux-gnu.tar.gz\n",
        "    !tar -xzf sage-v0.14.7-x86_64-unknown-linux-gnu.tar.gz && rm sage-v0.14.7-x86_64-unknown-linux-gnu.tar.gz\n",
        "    pip_packages = [\"casanovo==4.3.0\", \"biopython==1.85\", \"pyteomics==4.7.5\", \"mokapot==0.10.0\", \"numpy==1.26.4\", \"pandas==2.1.4\", \"xgboost==3.0.4\"]\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
        "                           *pip_packages])\n",
        "    Path(\"Orthrus_READY\").touch()\n",
        "else:\n",
        "    print(\"Environment already prepared.\")\n",
        "\n",
        "msg = \"Restarting ü´® ‚û°Ô∏è Don't click any Google prompts\"\n",
        "print(msg, flush=True)\n",
        "time.sleep(0.5)\n",
        "IPython.Application.instance().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "B5Kf77OUBFiN",
        "outputId": "84aa926a-175a-4063-cd87-84b050e2ca21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing conda and packagesüì¶ ‚¨áÔ∏è\n",
            "Restarting ü´® ‚û°Ô∏è Don't click any Google prompts\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure `Casanovo`\n",
        "#@markdown **`Casanovo` inputs**\n",
        "folder_path=\"\"#@param {type:\"string\"}\n",
        "#@markdown - a folder contains single or multiple `.mzML` or `.mgf` files for `Casanovo`. Please check only _ (underscore) and no other special characters or space in a file name. Ensure all instrument files in a single folder\n",
        "file_type=\"mzML\" #@param [\"mzML\", \"mgf\"]\n",
        "#@markdown - use the drop-down menu to choose the instrument file type\n",
        "\n",
        "use_default = False #@param {type:\"boolean\"}\n",
        "#@markdown - use the default model + configuration yaml from `Casanovo` github repo, may take time to download\n",
        "\n",
        "#@markdown **Advanced Options (user provided model + configuration yaml)**\n",
        "\n",
        "model = \"/content/drive/MyDrive/casanovo/models/casanovo4/ckpt/casanovo_massivekb_worker1.ckpt\" #@param {type:\"string\"}\n",
        "#@markdown - a `.ckpt` trained model (check point)\n",
        "config = \"/content/drive/MyDrive/casanovo/models/casanovo4/yaml/config_420.yaml\" #@param {type:\"string\"}\n",
        "#@markdown - a `.yaml` configuration file (see config_420_precursor_7_ppm.yaml)\n",
        "\n",
        "#@markdown **Inputs for converting Casanovo results to a `.fasta`**\n",
        "use_SwissProt = True #@param {type:\"boolean\"}\n",
        "#@markdown - use the latest, reviewed SwissProt form the UniProt FTP\n",
        "database_path=\"\"#@param {type:\"string\"}\n",
        "#@markdown - path to a user-defined database (`.fasta`)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WF6cl6QbBYGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure `SAGE`\n",
        "json_file_path = '/content/drive/MyDrive/casanovo/sage/config_general_MQ_fixed_CAM_v1.json' #@param {type:\"string\"}\n",
        "#@markdown - a configuration `.json` file (see config_general_MQ_fixed_CAM_v1.json)\n",
        "enzyme = \"KR\" #@param {type:\"string\"}\n",
        "#@markdown **`SAGE` PTM plus**\n",
        "#@markdown - Default `Sage` contains CAM (fixed) (+57.021464) + variable mods: Oxidation(M) (+15.994915), Deamidation(NQ) (+0.984016)\n",
        "#@markdown - PTM plus up to 5 variable mods and CAM (cysteine carbamidomethylation) can be turned off\n",
        "#@markdown - PTM mass can be any decimals\n",
        "use_PTM_plus = True #@param {type:\"boolean\"}\n",
        "static_CAM = True #@param {type:\"boolean\"}\n",
        "max_variable_mods= 3 #@param {type:\"number\"}\n",
        "#@markdown - please note `SAGE` only allow max 3 variable mods per PSM\n",
        "missed_cleavages= 2 #@param {type:\"number\"}\n",
        "AA_1 = \"M\" #@param [\"None\", \"[\",\"]\",\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "AA_1_mod = 15.9949 #@param {type:\"number\"}\n",
        "AA_2 = \"P\" #@param [\"None\", \"[\",\"]\",\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "AA_2_mod = 15.9949 #@param {type:\"number\"}\n",
        "AA_3 = \"N\" #@param [\"None\", \"[\",\"]\",\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "AA_3_mod = 0.984016 #@param {type:\"number\"}\n",
        "AA_4 = \"Q\" #@param [\"None\", \"[\",\"]\",\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "AA_4_mod = 0.984016 #@param {type:\"number\"}\n",
        "AA_5 = \"None\" #@param [\"None\", \"[\",\"]\",\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "#@markdown - [ = n-terminal\n",
        "AA_5_mod = \t42.010565 #@param {type:\"number\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SDNWI5r6Bhx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure Mokapot\n",
        "\n",
        "joint_modelling= True #@param {type:\"boolean\"}\n",
        "#@markdown - a joint model for low abundance samples, unclick for a separate model per experiment\n",
        "default_Percolator=True #@param {type:\"boolean\"}\n",
        "#@markdown - Python implementation of the Percolator SVM model\n",
        "#@markdown - the other option is the [XGBoost model](https://pubs.acs.org/doi/10.1021/acs.jproteome.0c01010)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E-6yPsj0BnFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import functions\n",
        "\n",
        "import re, glob, json, requests, gzip, shutil, os, subprocess, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyteomics import mztab\n",
        "from numpy import string_\n",
        "from joblib import Parallel, delayed\n",
        "from itertools import chain\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "\n",
        "'''\n",
        "parse a .mztab file using pyteomics\n",
        "add naked sequences (without PTMs) & the sequence length\n",
        "input=path to .mztab file\n",
        "output=pandas dataframe\n",
        "\n",
        "'''\n",
        "pattern = re.compile(r'(.\\d*\\.?\\d+)')\n",
        "\n",
        "def prep_mztab(mztab_path):\n",
        "  m = mztab.MzTab(mztab_path)\n",
        "  df = m.spectrum_match_table\n",
        "  if df is None or df.empty:\n",
        "    raise ValueError(f\"{mztab_path} is empty\")\n",
        "  if 'sequence' not in df.columns:\n",
        "    raise KeyError(f\"'sequence' column is missing in the file: {mztab_path}\")\n",
        "  df.reset_index(drop=True)\n",
        "  df1 = df.assign(sequence_naked=df['sequence'].str.replace(pattern, '', regex=True))\n",
        "  df2= df1.assign(nAA=df1['sequence_naked'].str.len())\n",
        "  df3=df2.sort_values(by='sequence_naked').drop_duplicates(subset='sequence_naked', keep=\"first\").reset_index(drop=True)\n",
        "  return df3\n",
        "\n",
        "'''\n",
        "parse a .fasta file using biopython\n",
        "add UniProt ID e.g. P02754\n",
        "input=path to .fasta file\n",
        "output=pandas dataframe\n",
        "\n",
        "'''\n",
        "\n",
        "def fasta_to_df(fasta_file):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "\n",
        "    protein_id = record.id\n",
        "    description = record.description\n",
        "    sequence = str(record.seq)\n",
        "    if not sequence:\n",
        "      raise ValueError(f\"Record with ID '{protein_id}' has no sequence in the fasta file.\")\n",
        "\n",
        "    data.append((protein_id, description, sequence))\n",
        "\n",
        "  df = pd.DataFrame(data, columns=[\"Protein_ID\", \"Description\", \"Sequence\"])\n",
        "  df1=df.assign(UniProt_ID=df['Protein_ID'].str.split('|').str[1])\n",
        "\n",
        "  return df1\n",
        "\n",
        "\n",
        "'''\n",
        "filter a Casanovo output file based on the maximum value below 0\n",
        "search_engine_score[1] is a score assigned to each prediction by Casanovo, max=1,\n",
        "if negative then outside the mass tolerance\n",
        "input=pandas dataframe\n",
        "output=pandas dataframe\n",
        "\n",
        "'''\n",
        "\n",
        "def casa_filter (df):\n",
        "  np_array = df['search_engine_score[1]'].to_numpy()\n",
        "  max_below_zero = np_array[np_array < 0].max()\n",
        "  df1=df[df['search_engine_score[1]']>=max_below_zero]\n",
        "  return df1\n",
        "\n",
        "\n",
        "#prepare overlapping sequence tags for string matching\n",
        "def get_seq_tags (sequence, k):\n",
        "  return set(sequence[i:i+k] for i in range(len(sequence) - k + 1))\n",
        "\n",
        "\n",
        "'''\n",
        "match de novo-based tags with database tags\n",
        "I=L in a reference database\n",
        "inputs=path to fasta, filtered casanovo output dataframe, tag size=k, chunk size=10000 for processing\n",
        "output=pandas dataframe\n",
        "\n",
        "'''\n",
        "\n",
        "def matching_count_v5 (df, df1, k, chunk_size=10000):\n",
        "\n",
        "  sequence_set = get_seq_tags(''.join(chain.from_iterable(df1['sequence_naked'].astype(str))), k)\n",
        "  print(f\"üìù {len(sequence_set)} tags regenerated. Starting matching...\")\n",
        "  result_df = pd.DataFrame()\n",
        "  for start in range(0, len(df), chunk_size):\n",
        "    chunk = df.iloc[start:start+chunk_size].copy()\n",
        "    chunk['seq_tags'] = chunk['Sequence'].astype(str).str.replace('I', 'L').apply(lambda x: get_seq_tags(x, k))\n",
        "    chunk['matched_count'] = chunk['seq_tags'].apply(lambda seq_tags: len(seq_tags & sequence_set))\n",
        "    chunk = chunk.assign(matched=chunk['matched_count'].apply(lambda x: 1 if x >= 2 else 0))\n",
        "    result_df = pd.concat([result_df, chunk], ignore_index=True)\n",
        "  total_matches=result_df['matched_count'].sum()\n",
        "  print(f\"Completed! {total_matches} matched üëç \")\n",
        "  return result_df\n",
        "\n",
        "#get tryptic peptides per database entry\n",
        "def count_tryptic_peptides(sequence):\n",
        "  pattern=r'(?<=[KR])'\n",
        "\n",
        "  peptides = re.split(pattern, sequence)\n",
        "\n",
        "  filtered_peptides = [peptide for peptide in peptides if len(peptide) >= 6]\n",
        "\n",
        "  return len(filtered_peptides)\n",
        "\n",
        "#prepare a dataframe for NB classification\n",
        "def prep_Bayes (df):\n",
        "  print('üßë‚Äçüíª Start Bayes probabilistic ranking...')\n",
        "  df1=df.assign(length=df['Sequence'].astype(str).str.len(),\n",
        "                 tryptic_count=df['Sequence'].apply(count_tryptic_peptides),\n",
        "                 tag_count=df['seq_tags'].apply(len))\n",
        "  df2=df1.assign(SAF=df1['matched_count']/df1['length'],\n",
        "                 try_ratio=df1['tryptic_count']/df1['tag_count']\n",
        "                 )\n",
        "  return df2\n",
        "\n",
        "\n",
        "'''\n",
        "ranking matched proteins based on SAF and try_ratio\n",
        "values normalised before NB classification\n",
        "most probable matches (>= 95%) are shortlised\n",
        "\n",
        "'''\n",
        "def get_bayes_ranking_test (df, threshold=0.95):\n",
        "  m=prep_Bayes(df)\n",
        "  required_columns = {'SAF', 'try_ratio', 'matched'}\n",
        "  if not required_columns.issubset(m.columns):\n",
        "    missing_cols = required_columns - set(m.columns)\n",
        "    raise ValueError(f\"Missing columns in DataFrame: {missing_cols}\")\n",
        "  m1=m[m['tag_count']>0]\n",
        "  X = m1[['SAF', 'try_ratio']].to_numpy()\n",
        "  y = m1['matched'].to_numpy()\n",
        "  scaler = MinMaxScaler()\n",
        "  X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(*X.shape)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=7)\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(X_train, y_train)\n",
        "  y_pred = gnb.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  f1=f1_score(y_test, y_pred)\n",
        "  print(f\"‚úÖ Gaussian Naive Bayes model ‚ñ∂Ô∏è accuracy:{accuracy:.4f}, precision:{precision:.4f}, f1:{f1:.4f}\")\n",
        "  whole_pred=gnb.predict(X_scaled)\n",
        "  class_probabilities = gnb.predict_proba(X_scaled)\n",
        "  m2=m1.assign(pred=class_probabilities[:, 1])\n",
        "  m3=m2[m2['pred']>=threshold]\n",
        "  return m3\n",
        "\n",
        "\n",
        "#combine previous functions together to output a shortlisted .fasta\n",
        "\n",
        "def matching_ranking_to_fasta_v5 (mztab_path, fasta_df):\n",
        "  p=prep_mztab(mztab_path)\n",
        "  p1=casa_filter(p)\n",
        "  k=int(p1['nAA'].median())\n",
        "  m=matching_count_v5 (fasta_df, p1, k, chunk_size=10000)\n",
        "  m1=get_bayes_ranking_test (m)\n",
        "  seq_records = []\n",
        "  for index, row in m1.iterrows():\n",
        "    header_id = f\"{row['Description']}\"\n",
        "    sequence = Seq(row['Sequence'])\n",
        "    description = \"\"\n",
        "    seq_record = SeqRecord(sequence, id=header_id, description=description)\n",
        "    seq_records.append(seq_record)\n",
        "\n",
        "  output_fasta_filepath = mztab_path.replace('.mztab', '_matched.fasta')\n",
        "\n",
        "  with open(output_fasta_filepath, 'w') as output_file:\n",
        "    SeqIO.write(seq_records, output_file, 'fasta')\n",
        "  print(f\"üéä Number of protein entries in the output fasta: {m1.shape[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "#generate a de novo-first, experiment-specific .fasta for each input\n",
        "def process_all_mztab_files_v2 (folder_path, database_path):\n",
        "    mztab_filepaths = glob.glob(f\"{folder_path}/*.mztab\")\n",
        "    print(f\"üóÇÔ∏è {len(mztab_filepaths)} file(s) collecting from {folder_path}...\")\n",
        "    fas=fasta_to_df(database_path)\n",
        "    fasta_df=pd.DataFrame.from_dict(fas)\n",
        "    print(f\"‚¨ÜÔ∏è {database_path} loaded\")\n",
        "    print(f\"üì§ No. of proteins in the reference fasta: {fasta_df.shape[0]}\")\n",
        "\n",
        "    for mztab in mztab_filepaths:\n",
        "      print(f\"üöÄ Processing file: {mztab}\")\n",
        "      matching_ranking_to_fasta_v5 (mztab, fasta_df)\n",
        "\n",
        "'''\n",
        "Organise mztabs and instrument files in the same folder\n",
        "\n",
        "'''\n",
        "\n",
        "def organise_files (directory):\n",
        "    print(\"Organising files...\")\n",
        "\n",
        "    if not os.path.isdir(directory):\n",
        "        print(f\"The directory {directory} does not exist.\")\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    MS2_files = glob.glob(os.path.join(directory, f'*.{file_type}'))\n",
        "\n",
        "    for MS2 in MS2_files:\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(MS2))[0]\n",
        "\n",
        "        new_folder_path = os.path.join(directory, base_name)\n",
        "\n",
        "        if not os.path.exists(new_folder_path):\n",
        "            os.makedirs(new_folder_path)\n",
        "            print(f\"Created folder: {new_folder_path}\")\n",
        "        else:\n",
        "            print(f\"Folder already exists: {new_folder_path}\")\n",
        "\n",
        "\n",
        "        MS2_path = os.path.join(new_folder_path, os.path.basename(MS2))\n",
        "        if not os.path.exists(MS2_path):\n",
        "            shutil.move(MS2, new_folder_path)\n",
        "            print(f\"Moved {MS2} to {new_folder_path}\")\n",
        "        else:\n",
        "            print(f\"MS2 file already exists in the destination: {MS2_path}\")\n",
        "\n",
        "\n",
        "        fasta_filename = f\"{base_name}_matched.fasta\"\n",
        "        fasta_file = os.path.join(directory, fasta_filename)\n",
        "\n",
        "\n",
        "        if os.path.exists(fasta_file):\n",
        "            new_fasta_path = os.path.join(new_folder_path, fasta_filename)\n",
        "            if not os.path.exists(new_fasta_path):\n",
        "                shutil.move(fasta_file, new_folder_path)\n",
        "                print(f\"Moved {fasta_file} to {new_folder_path}\")\n",
        "            else:\n",
        "                print(f\".fasta file already exists in the destination: {new_fasta_path}\")\n",
        "        else:\n",
        "            print(f\"No matching .fasta file found for {base_name}\")\n",
        "\n",
        "\n",
        "\"\"\"Generate and save a Sage configuration .json file.\"\"\"\n",
        "\n",
        "def get_sage_config(json_file_path, peak_folder, static_mods, new_mods,\n",
        "                    missed_cleavages, enzyme,\n",
        "                    min_len, max_len, max_variable_mods,\n",
        "                    output_config_path):\n",
        "\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "        peak_files = glob.glob(peak_folder)\n",
        "        print(f\"üóÇÔ∏è {len(peak_files)} file(s) collected from {peak_folder}\")\n",
        "\n",
        "        json_data['mzml_paths'] = peak_files\n",
        "        json_data['database']['static_mods'] = static_mods\n",
        "        json_data['database']['variable_mods'] = new_mods\n",
        "        json_data['database']['enzyme']['missed_cleavages'] = missed_cleavages\n",
        "        json_data['database']['enzyme']['cleave_at']= enzyme\n",
        "        json_data['database']['enzyme']['min_len'] = min_len\n",
        "        json_data['database']['enzyme']['max_len'] = max_len\n",
        "        json_data['database']['max_variable_mods'] = max_variable_mods\n",
        "        json_data['database']['decoy_tag'] = \"rev_\"\n",
        "        json_data['database']['generate_decoys'] = True\n",
        "\n",
        "    with open(output_config_path, 'w') as f:\n",
        "        json.dump(json_data, f, indent=4)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3CTRvCKVBqnN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Casanovo\n",
        "\n",
        "if use_default:\n",
        "  folder = glob.glob(f\"{folder_path}/*.{file_type}\")\n",
        "  for instrument_file in folder:\n",
        "    output_path=instrument_file.replace(f\".{file_type}\", \".mztab\")\n",
        "    ! casanovo sequence {instrument_file} -v info -o {output_path}\n",
        "\n",
        "else:\n",
        "  folder = glob.glob(f\"{folder_path}/*.{file_type}\")\n",
        "  for instrument_file in folder:\n",
        "    output_path=instrument_file.replace(f\".{file_type}\", \".mztab\")\n",
        "    ! casanovo sequence {instrument_file} -m {model} -c {config} -v info -o {output_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SCGtRiwZBw8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert `Casanovo` results to .fasta per experiment\n",
        "\n",
        "if use_SwissProt:\n",
        "  url = \"https://ftp.uniprot.org/pub/databases/uniprot/knowledgebase/complete/uniprot_sprot.fasta.gz\"\n",
        "  output_file = \"uniprot_sprot.fasta.gz\"\n",
        "  decompressed_file = \"uniprot_sprot.fasta\"\n",
        "  response = requests.get(url, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(output_file, 'wb') as f:\n",
        "      shutil.copyfileobj(response.raw, f)\n",
        "    print(f\"{output_file} downloaded successfully.\")\n",
        "  else:\n",
        "    print(f\"Failed to download {output_file}, status code: {response.status_code}\")\n",
        "  with gzip.open(output_file, 'rb') as f_in:\n",
        "    with open(decompressed_file, 'wb') as f_out:\n",
        "      shutil.copyfileobj(f_in, f_out)\n",
        "  sprot_path=\"uniprot_sprot.fasta\"\n",
        "  process_all_mztab_files_v2(folder_path, sprot_path)\n",
        "\n",
        "else:\n",
        "  process_all_mztab_files_v2(folder_path, database_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9G6TP8VwByxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Sage\n",
        "\n",
        "organise_files(folder_path)\n",
        "folder_path = folder_path\n",
        "#colab version\n",
        "sage_path=\"/content/sage-v0.14.7-x86_64-unknown-linux-gnu/sage\"\n",
        "\n",
        "if use_PTM_plus:\n",
        "    AAs = [AA_1, AA_2, AA_3, AA_4, AA_5]\n",
        "    mods = [AA_1_mod, AA_2_mod, AA_3_mod, AA_4_mod, AA_5_mod]\n",
        "    PTMs = {}\n",
        "\n",
        "    for AA, mod in zip(AAs, mods):\n",
        "        if AA != \"None\":\n",
        "            PTMs[AA] = [mod]\n",
        "\n",
        "    big_folder = glob.glob(f\"{folder_path}/*\")\n",
        "    for folder in big_folder:\n",
        "        if not os.path.isdir(folder):\n",
        "            continue\n",
        "\n",
        "        mzml_files = glob.glob(f\"{folder}/*.{file_type}\")\n",
        "        if not mzml_files:\n",
        "            continue\n",
        "\n",
        "        peak_path = mzml_files[0]\n",
        "        output_json = peak_path.replace(f\".{file_type}\", '.json')\n",
        "\n",
        "        json_file_path = json_file_path\n",
        "        missed_cleavages = missed_cleavages\n",
        "        enzyme = enzyme\n",
        "        min_len = 6\n",
        "        max_len = 30\n",
        "        max_variable_mods = max_variable_mods\n",
        "        static_mods = {\"C\": 57.021464} if static_CAM else {}\n",
        "\n",
        "        get_sage_config(\n",
        "            json_file_path, peak_path, static_mods, PTMs,\n",
        "            missed_cleavages, enzyme,\n",
        "            min_len, max_len,\n",
        "            max_variable_mods, output_json\n",
        "        )\n",
        "\n",
        "        fasta_files = glob.glob(f\"{folder}/*.fasta\")\n",
        "        if not fasta_files:\n",
        "            continue\n",
        "\n",
        "        fasta_path = fasta_files[0]\n",
        "        # path to Sage binary\n",
        "        ! {sage_path} {output_json} --fasta {fasta_path} \\\n",
        "            --write-pin --output_directory {folder}\n",
        "\n",
        "else:\n",
        "    big_folder = glob.glob(f\"{folder_path}/*\")\n",
        "    for folder in big_folder:\n",
        "        if not os.path.isdir(folder):\n",
        "            continue\n",
        "\n",
        "        mzml_files = glob.glob(f\"{folder}/*.{file_type}\")\n",
        "        if not mzml_files:\n",
        "            continue\n",
        "\n",
        "        peak_path = mzml_files[0]\n",
        "        output_json = peak_path.replace(f\".{file_type}\", '.json')\n",
        "        json_file_path = json_file_path\n",
        "        missed_cleavages = 2\n",
        "        enzyme = enzyme\n",
        "        min_len = 6\n",
        "        max_len = 30\n",
        "        max_variable_mods = 5\n",
        "        static_mods = {\"C\": 57.021464}\n",
        "        new_mods = {\"M\": [15.994915], \"N\": [0.984016], \"Q\": [0.984016]}\n",
        "\n",
        "        get_sage_config(\n",
        "            json_file_path, peak_path, static_mods, new_mods,\n",
        "            missed_cleavages, enzyme,\n",
        "            min_len, max_len,\n",
        "            max_variable_mods, output_json\n",
        "        )\n",
        "\n",
        "        fasta_files = glob.glob(f\"{folder}/*.fasta\")\n",
        "        if not fasta_files:\n",
        "            continue\n",
        "\n",
        "        fasta_path = fasta_files[0]\n",
        "        # path to Sage binary\n",
        "        ! {sage_path} {output_json} --fasta {fasta_path} \\\n",
        "            --write-pin --output_directory {folder}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "V18lOwBdB2An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Brew Mokapot\n",
        "\n",
        "import mokapot\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "\"\"\"\n",
        "XGBoost schema from Fondrie & Noble (2021).\n",
        "A non-linear XGBoost seems to be better for rescoring open search results.\n",
        "\"\"\"\n",
        "\n",
        "#from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "grid = {\n",
        "    \"scale_pos_weight\": np.logspace(0, 2, 3),\n",
        "    \"max_depth\": [1, 3, 6],\n",
        "    \"min_child_weight\": [1, 10, 100],\n",
        "    \"gamma\": [0, 1, 10],\n",
        "}\n",
        "\n",
        "\n",
        "xgb_mod = GridSearchCV(\n",
        "    XGBClassifier(),\n",
        "    param_grid=grid,\n",
        "    n_jobs=1,\n",
        "    cv=3,\n",
        "    scoring=\"roc_auc\",\n",
        ")\n",
        "\n",
        "\"\"\"Recursively find all .pin files in the given folder.\"\"\"\n",
        "def get_all_pin_files(folder_path):\n",
        "    psm_files = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.pin'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                psm_files.append(full_path)\n",
        "    return psm_files\n",
        "\n",
        "\n",
        "if joint_modelling:\n",
        "     psm_files = get_all_pin_files(folder_path)\n",
        "     if default_Percolator:\n",
        "        svm = mokapot.PercolatorModel()\n",
        "        psm_list = mokapot.read_pin(psm_files)\n",
        "        results, models = mokapot.brew(psm_list, svm)\n",
        "        result_files = results.to_txt(folder_path)\n",
        "     else:\n",
        "        mod = mokapot.Model(xgb_mod)\n",
        "        psm_list = mokapot.read_pin(psm_files)\n",
        "        results, models = mokapot.brew(psm_list, mod)\n",
        "        result_files = results.to_txt(folder_path)\n",
        "\n",
        "else:\n",
        "    big_folder = sorted(glob.glob(f\"{folder_path}/*\"))\n",
        "\n",
        "    for folder in big_folder:\n",
        "        if not os.path.isdir(folder):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing folder: {folder}\")\n",
        "        pin_files = glob.glob(f\"{folder}/*.pin\")\n",
        "\n",
        "        if not pin_files:\n",
        "            print(f\"No .pin files found in {folder}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        pin = pin_files[0]\n",
        "\n",
        "        if default_Percolator:\n",
        "            svm = mokapot.PercolatorModel()\n",
        "            psm_list = mokapot.read_pin(pin)\n",
        "            results, models = mokapot.brew(psm_list, svm)\n",
        "            result_files = results.to_txt(folder)\n",
        "        else:\n",
        "            mod = mokapot.Model(xgb_mod)\n",
        "            psm_list = mokapot.read_pin(pin)\n",
        "            results, models = mokapot.brew(psm_list, mod)\n",
        "            result_files = results.to_txt(folder)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sPPjHeaPB8O6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}